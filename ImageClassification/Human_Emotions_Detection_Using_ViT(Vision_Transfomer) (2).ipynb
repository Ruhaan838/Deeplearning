{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db4HAHW_kqXw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, InputLayer,BatchNormalization,Input,Layer,Dropout,Resizing,Rescaling,RandomRotation,RandomFlip,RandomContrast,Embedding,LayerNormalization,MultiHeadAttention,Add,Permute\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy , FalsePositives,FalseNegatives,TruePositives,TrueNegatives,Precision,Recall,AUC,CategoricalAccuracy,TopKCategoricalAccuracy\n",
        "from tensorflow.keras.callbacks import Callback,CSVLogger,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau,TensorBoard\n",
        "from tensorflow.keras.regularizers import L1,L2\n",
        "from tensorflow.image import flip_left_right,random_flip_up_down,rot90,adjust_brightness,random_saturation,central_crop,adjust_saturation,crop_to_bounding_box,pad_to_bounding_box\n",
        "from tensorflow.train import BytesList,FloatList,Int64List,Example,Features,Feature\n",
        "import sklearn as sl\n",
        "from sklearn.metrics import confusion_matrix,roc_curve\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "uJ_G3fnolU6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "fFuJ1pF6lxDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "5ExIcOPzlxSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d muhammadhananasghar/human-emotions-datasethes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0vBUmvIlxWm",
        "outputId": "64174f8a-7709-4dcf-9899-5c67dfac74f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading human-emotions-datasethes.zip to /content\n",
            " 97% 299M/309M [00:02<00:00, 120MB/s] \n",
            "100% 309M/309M [00:02<00:00, 115MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/human-emotions-datasethes.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "id": "oNbLmLuMlxZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dic = \"/content/dataset/Emotions Dataset/Emotions Dataset/train\"\n",
        "val_dic = \"/content/dataset/Emotions Dataset/Emotions Dataset/test\"\n",
        "CONFIGURATION = {\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"IMG_SIZE\": 256,\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "\n",
        "    \"N_EPOCHS\": 20,\n",
        "    \"DROPOUT_RATE\": 0.0,\n",
        "    \"REGULARIZATION_RATE\": 0.0,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"NUM_CLASSES\": 3,\n",
        "    \"PATCH_SIZE\": 16,\n",
        "    \"PROJ_DIM\": 768,\n",
        "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
        "}"
      ],
      "metadata": {
        "id": "5G6c9UYQlxcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dic,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
        "    color_mode='rgb',\n",
        "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
        "    image_size=(CONFIGURATION[\"IMG_SIZE\"], CONFIGURATION[\"IMG_SIZE\"]),\n",
        "    shuffle=True,\n",
        "    seed=99,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AddkDtaXUaR",
        "outputId": "ddf51fa4-1cc0-42da-d121-08a5e12a7fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6799 files belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_dic,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
        "    color_mode='rgb',\n",
        "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
        "    image_size=(CONFIGURATION[\"IMG_SIZE\"], CONFIGURATION[\"IMG_SIZE\"]),\n",
        "    shuffle=True,\n",
        "    seed=99,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CFmPBkrXZOk",
        "outputId": "2425f6ea-fbda-4ea5-a263-cbfae71f091e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2278 files belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = (\n",
        "    train_data.prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "t9B98YWbXuEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = (\n",
        "    val_data\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "TSLHzhRiX0mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(Layer):\n",
        "  def __init__ (self,N_PATCHES,HIDDEN_SIZE):\n",
        "    super(PatchEncoder,self).__init__(name= \"Patch_Encoder\")\n",
        "    self.liner_projection = Dense(HIDDEN_SIZE)\n",
        "    self.Embedding_projection = Embedding(N_PATCHES,HIDDEN_SIZE)\n",
        "    self.N_PATCHES = N_PATCHES\n",
        "  def call(self,x):\n",
        "    patches = tf.image.extract_patches(\n",
        "              images=x,\n",
        "              sizes = [1,CONFIGURATION[\"PATCH_SIZE\"],CONFIGURATION[\"PATCH_SIZE\"],1],\n",
        "              strides = [1,CONFIGURATION[\"PATCH_SIZE\"],CONFIGURATION[\"PATCH_SIZE\"],1],\n",
        "              rates = [1,1,1,1],\n",
        "              padding='VALID')\n",
        "    patches = tf.reshape(patches,(tf.shape(patches)[0],-1,patches.shape[-1]))\n",
        "    embadding_input = tf.range(0,self.N_PATCHES,1)\n",
        "    output = self.liner_projection(patches) + self.Embedding_projection(embadding_input)\n",
        "    return output"
      ],
      "metadata": {
        "id": "CsO3SMlJX239"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(Layer):\n",
        "  def __init__ (self,N_HEADS,HIDDEN_SIZE):\n",
        "    super(TransformerEncoder,self).__init__(name=\"Transfomer_Encoder\")\n",
        "\n",
        "    self.layer_norm_1 = LayerNormalization()\n",
        "    self.layer_norm_2 = LayerNormalization()\n",
        "\n",
        "    self.mult_head_att = MultiHeadAttention(N_HEADS,HIDDEN_SIZE)\n",
        "\n",
        "    self.dense_1 = Dense(HIDDEN_SIZE,activation = tf.nn.gelu)\n",
        "    self.dense_2 = Dense(HIDDEN_SIZE,activation = tf.nn.gelu)\n",
        "  def call(self,input):\n",
        "    x_1 = self.layer_norm_1(input)\n",
        "    x_1 = self.mult_head_att(x_1,x_1)\n",
        "\n",
        "    x_1 = Add()([x_1,input])\n",
        "\n",
        "    x_1 = self.layer_norm_2(x_1)\n",
        "\n",
        "    x_2 = self.dense_1(x_1)\n",
        "    x_2 = self.dense_2(x_2)\n",
        "    output = Add()([x_2,x_1])\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "MadSI9PGbaV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(Model):\n",
        "  def __init__(self,N_HEADS,HIDDEN_SIZE,N_PATCHES,N_LAYERS,N_DENSE_UNITS):\n",
        "    super(ViT,self).__init__(name=\"ViT\")\n",
        "    self.patch_encoder = PatchEncoder(N_PATCHES,HIDDEN_SIZE)\n",
        "    self.trans_encoder = [TransformerEncoder(N_HEADS,HIDDEN_SIZE) for _ in range(N_LAYERS)]\n",
        "    self.N_LAYERS = N_LAYERS\n",
        "    self.dense_1 = Dense(N_DENSE_UNITS,activation =tf.nn.gelu )\n",
        "    self.dense_2 = Dense(N_DENSE_UNITS,activation =tf.nn.gelu )\n",
        "    self.dense_3 = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = 'softmax')\n",
        "  def call(self,input,training = True):\n",
        "\n",
        "    x = self.patch_encoder(input)\n",
        "\n",
        "    for i in range(self.N_LAYERS):\n",
        "      x = self.trans_encoder[i](x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = self.dense_1(x)\n",
        "    x = self.dense_2(x)\n",
        "\n",
        "    return self.dense_3(x)"
      ],
      "metadata": {
        "id": "cEjI_ywjXV2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit  = ViT(N_HEADS = 8,HIDDEN_SIZE = 768,N_PATCHES = 256,N_LAYERS = 4 ,N_DENSE_UNITS=1024)\n",
        "vit(tf.zeros([32,256,256,3]))"
      ],
      "metadata": {
        "id": "RhtwmDa5b4wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31ba485-75b0-495e-c251-47bd69b938dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32, 3), dtype=float32, numpy=\n",
              "array([[0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922],\n",
              "       [0.45988318, 0.3327576 , 0.20735922]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89aisg5UcLnt",
        "outputId": "94c48b00-16c6-441b-8e03-487bd622ab0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ViT\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Patch_Encoder (PatchEncode  multiple                  787200    \n",
            " r)                                                              \n",
            "                                                                 \n",
            " Transfomer_Encoder (Transf  multiple                  20077824  \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " Transfomer_Encoder (Transf  multiple                  20077824  \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " Transfomer_Encoder (Transf  multiple                  20077824  \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " Transfomer_Encoder (Transf  multiple                  20077824  \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " dense_9 (Dense)             multiple                  201327616 \n",
            "                                                                 \n",
            " dense_10 (Dense)            multiple                  1049600   \n",
            "                                                                 \n",
            " dense_11 (Dense)            multiple                  3075      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 283478787 (1.06 GB)\n",
            "Trainable params: 283478787 (1.06 GB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit.compile(\n",
        "    optimizer = Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "    loss = CategoricalCrossentropy(),\n",
        "    metrics = 'accuracy'\n",
        ")"
      ],
      "metadata": {
        "id": "w8CAnt5kc81G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = vit.fit(\n",
        "    train_data,\n",
        "    epochs = CONFIGURATION[\"N_EPOCHS\"],\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "Oq-51WqCfG1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673e4cdd-78cf-4ae1-e51e-8776bcff2b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "213/213 [==============================] - 326s 1s/step - loss: 4811.0635 - accuracy: 0.3951\n",
            "Epoch 2/20\n",
            "213/213 [==============================] - 312s 1s/step - loss: 3.5771 - accuracy: 0.4434\n",
            "Epoch 3/20\n",
            "213/213 [==============================] - 304s 1s/step - loss: 1.2064 - accuracy: 0.4483\n",
            "Epoch 4/20\n",
            "213/213 [==============================] - 304s 1s/step - loss: 1.0783 - accuracy: 0.4512\n",
            "Epoch 5/20\n",
            "213/213 [==============================] - 303s 1s/step - loss: 1.0803 - accuracy: 0.4483\n",
            "Epoch 6/20\n",
            "213/213 [==============================] - 303s 1s/step - loss: 1.0659 - accuracy: 0.4484\n",
            "Epoch 7/20\n",
            "213/213 [==============================] - 303s 1s/step - loss: 1.1100 - accuracy: 0.4486\n",
            "Epoch 8/20\n",
            "213/213 [==============================] - 296s 1s/step - loss: 1.1332 - accuracy: 0.4501\n",
            "Epoch 9/20\n",
            "213/213 [==============================] - 305s 1s/step - loss: 26771.5508 - accuracy: 0.4220\n",
            "Epoch 10/20\n",
            "213/213 [==============================] - 301s 1s/step - loss: 1.0622 - accuracy: 0.4440\n",
            "Epoch 11/20\n",
            "213/213 [==============================] - 301s 1s/step - loss: 1.0622 - accuracy: 0.4440\n",
            "Epoch 12/20\n",
            "213/213 [==============================] - 301s 1s/step - loss: 1.0624 - accuracy: 0.4440\n",
            "Epoch 13/20\n",
            "213/213 [==============================] - 301s 1s/step - loss: 1.0625 - accuracy: 0.4440\n",
            "Epoch 14/20\n",
            "213/213 [==============================] - 300s 1s/step - loss: 1.0625 - accuracy: 0.4440\n",
            "Epoch 15/20\n",
            "213/213 [==============================] - 301s 1s/step - loss: 1.0626 - accuracy: 0.4440\n",
            "Epoch 16/20\n",
            "213/213 [==============================] - 300s 1s/step - loss: 1.0626 - accuracy: 0.4440\n",
            "Epoch 17/20\n",
            "209/213 [============================>.] - ETA: 5s - loss: 1.0621 - accuracy: 0.4447"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "nEM0Xip4fp6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_resacle_hf_vit = Sequential([\n",
        "    Resizing(224,224),\n",
        "    Rescaling(1./255),\n",
        "    Permute((3,1,2))\n",
        "])"
      ],
      "metadata": {
        "id": "N2UFT9TwmDMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, TFViTModel\n",
        "\n",
        "base_model = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "inputs = Input(shape= (256,256,3))\n",
        "x = resize_resacle_hf_vit(inputs)\n",
        "x = base_model.vit(x)[0][:,0,:]\n",
        "output = Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = 'softmax')(x)\n",
        "hf_vit_model = tf.keras.Model(inputs = inputs,outputs = output)"
      ],
      "metadata": {
        "id": "41VOZqKej7fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_vit_model.summary()"
      ],
      "metadata": {
        "id": "8sD0tgdgnvn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_vit_model.compile(\n",
        "    optimizer = Adam(learning_rate=5e-5),\n",
        "    loss = CategoricalCrossentropy(),\n",
        "    metrics = 'accuracy'\n",
        ")"
      ],
      "metadata": {
        "id": "w8N072xwpvds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = hf_vit_model.fit(\n",
        "    train_data,\n",
        "    epochs = 10,\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "JblL23d7qb93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8876811d-ba4b-436e-f91f-9625b424ae90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "213/213 [==============================] - 94s 440ms/step - loss: 0.6764 - accuracy: 0.7292\n",
            "Epoch 2/10\n",
            "213/213 [==============================] - 94s 439ms/step - loss: 0.6657 - accuracy: 0.7342\n",
            "Epoch 3/10\n",
            "213/213 [==============================] - 94s 439ms/step - loss: 0.6563 - accuracy: 0.7389\n",
            "Epoch 4/10\n",
            "213/213 [==============================] - 94s 439ms/step - loss: 0.6480 - accuracy: 0.7394\n",
            "Epoch 5/10\n",
            "213/213 [==============================] - 94s 439ms/step - loss: 0.6405 - accuracy: 0.7408\n",
            "Epoch 6/10\n",
            "213/213 [==============================] - 94s 440ms/step - loss: 0.6336 - accuracy: 0.7432\n",
            "Epoch 7/10\n",
            "213/213 [==============================] - 94s 439ms/step - loss: 0.6275 - accuracy: 0.7439\n",
            "Epoch 8/10\n",
            "213/213 [==============================] - 94s 440ms/step - loss: 0.6218 - accuracy: 0.7448\n",
            "Epoch 9/10\n",
            "213/213 [==============================] - 94s 440ms/step - loss: 0.6167 - accuracy: 0.7466\n",
            "Epoch 10/10\n",
            "213/213 [==============================] - 94s 439ms/step - loss: 0.6118 - accuracy: 0.7492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21w94_ZTq6Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cN5NWP42FM4v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}